{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed5981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame reader finished.\n",
      "Frame processor finished.\n",
      "Exiting main display loop.\n",
      "Program finished.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "BUFFER_SIZE = 5  # Number of frames to buffer between producer and consumer\n",
    "DISPLAY_FPS = 30 # Target FPS for display. Setting this too high might cause stuttering.\n",
    "\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Queues for inter-thread communication\n",
    "frame_queue = queue.Queue(maxsize=BUFFER_SIZE)\n",
    "processed_frame_queue = queue.Queue(maxsize=BUFFER_SIZE)\n",
    "\n",
    "# Event to signal when the video reading is done\n",
    "video_finished = threading.Event()\n",
    "processing_finished = threading.Event()\n",
    "\n",
    "# --- Producer Thread: Reads frames from video ---\n",
    "def frame_reader(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        video_finished.set()\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_queue.put(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    video_finished.set()\n",
    "    print(\"Frame reader finished.\")\n",
    "\n",
    "# --- Consumer Thread: Processes frames ---\n",
    "def frame_processor():\n",
    "    while True:\n",
    "        if video_finished.is_set() and frame_queue.empty():\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # This prevents the thread from blocking indefinitely if the producer is slow\n",
    "            # or has finished and there are no more frames.\n",
    "            frame = frame_queue.get(timeout=0.1) \n",
    "        except queue.Empty:\n",
    "            continue # No frame available, try again\n",
    "\n",
    "        frame_resized = cv2.resize(frame, (640, 480))\n",
    "\n",
    "        boxes, weights = hog.detectMultiScale(frame_resized, winStride=(8,8), scale=1.02, padding=(4,4))\n",
    "\n",
    "        for i, (x,y,w,h) in enumerate(boxes):\n",
    "            if weights[i] < 0.13:\n",
    "                continue\n",
    "            #elif weights[i] < 0.3 and weights[i] > 0.13:\n",
    "            #    cv2.rectangle(frame_resized, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "            #elif weights[i] < 0.7 and weights[i] > 0.3:\n",
    "            #    cv2.rectangle(frame_resized, (x, y), (x+w, y+h), (50, 122, 255), 2)\n",
    "            elif weights[i] > 0.7:\n",
    "                cv2.rectangle(frame_resized, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        cv2.putText(frame_resized, 'High', (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        #cv2.putText(frame_resized, 'Moderate', (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 122, 255), 2)\n",
    "        #cv2.putText(frame_resized, 'Low', (10, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "        processed_frame_queue.put(frame_resized)\n",
    "        frame_queue.task_done()\n",
    "    \n",
    "    processing_finished.set() # Signal that processing is complete\n",
    "    print(\"Frame processor finished.\")\n",
    "\n",
    "def main():\n",
    "    video_path = '../853889-hd_1920_1080_25fps.mp4'\n",
    "\n",
    "    # Start the producer thread\n",
    "    reader_thread = threading.Thread(target=frame_reader, args=(video_path,))\n",
    "    reader_thread.daemon = True # Allow the main program to exit even if this thread is running\n",
    "    reader_thread.start()\n",
    "\n",
    "    # Start the consumer thread\n",
    "    processor_thread = threading.Thread(target=frame_processor)\n",
    "    processor_thread.daemon = True\n",
    "    processor_thread.start()\n",
    "\n",
    "    last_display_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        if video_finished.is_set() and processing_finished.is_set() and processed_frame_queue.empty():\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Get processed frame for display\n",
    "            frame_to_display = processed_frame_queue.get(timeout=0.1) \n",
    "        except queue.Empty:\n",
    "            # If the queue is empty, wait a bit before trying again to avoid busy-waiting\n",
    "            time.sleep(0.01) \n",
    "            continue\n",
    "\n",
    "        cv2.imshow(\"Frame\", frame_to_display)\n",
    "        processed_frame_queue.task_done() # Indicate that a task from processed_frame_queue is done\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27: # ESC key to exit\n",
    "            break\n",
    "\n",
    "    print(\"Exiting main display loop.\")\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Program finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f82d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "def is_all_fingers_up(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Checks if all fingers (thumb, index, middle, ring, pinky) are extended upwards\n",
    "    in a high-five pose.\n",
    "\n",
    "    Args:\n",
    "        hand_landmarks: A MediaPipe HandLandmark object containing the 3D coordinates\n",
    "                        of the hand landmarks.\n",
    "\n",
    "    Returns:\n",
    "        True if all fingers are detected as up, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    if not hand_landmarks:\n",
    "        return False\n",
    "\n",
    "    # Define landmark indices for each finger and their corresponding MCP, PIP, DIP joints.\n",
    "    # Note: For the thumb, we primarily check the IP joint relative to the MCP/CMC.\n",
    "    # For other fingers, we check PIP and DIP relative to MCP.\n",
    "    fingers = {\n",
    "        'thumb': [mp_hands.HandLandmark.THUMB_CMC, mp_hands.HandLandmark.THUMB_MCP, mp_hands.HandLandmark.THUMB_IP, mp_hands.HandLandmark.THUMB_TIP],\n",
    "        'index': [mp_hands.HandLandmark.INDEX_FINGER_MCP, mp_hands.HandLandmark.INDEX_FINGER_PIP, mp_hands.HandLandmark.INDEX_FINGER_DIP, mp_hands.HandLandmark.INDEX_FINGER_TIP],\n",
    "        'middle': [mp_hands.HandLandmark.MIDDLE_FINGER_MCP, mp_hands.HandLandmark.MIDDLE_FINGER_PIP, mp_hands.HandLandmark.MIDDLE_FINGER_DIP, mp_hands.HandLandmark.MIDDLE_FINGER_TIP],\n",
    "        'ring': [mp_hands.HandLandmark.RING_FINGER_MCP, mp_hands.HandLandmark.RING_FINGER_PIP, mp_hands.HandLandmark.RING_FINGER_DIP, mp_hands.HandLandmark.RING_FINGER_TIP],\n",
    "        'pinky': [mp_hands.HandLandmark.PINKY_MCP, mp_hands.HandLandmark.PINKY_PIP, mp_hands.HandLandmark.PINKY_DIP, mp_hands.HandLandmark.PINKY_TIP],\n",
    "    }\n",
    "\n",
    "    # A threshold for the angle to determine if a finger is \"up\" (extended).\n",
    "    # This might need fine-tuning based on your camera angle and hand size.\n",
    "    # A smaller angle means more extended.\n",
    "    angle_threshold = 30  # degrees\n",
    "\n",
    "    all_fingers_extended = True\n",
    "\n",
    "    # Get wrist landmark to establish a reference point for \"up\"\n",
    "    wrist_y = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].y\n",
    "\n",
    "    for finger_name, landmarks in fingers.items():\n",
    "        if finger_name == 'thumb':\n",
    "            # For the thumb, check if the tip is significantly above the MCP,\n",
    "            # and the overall curl.\n",
    "            mcp = hand_landmarks.landmark[landmarks[1]]\n",
    "            ip = hand_landmarks.landmark[landmarks[2]]\n",
    "            tip = hand_landmarks.landmark[landmarks[3]]\n",
    "\n",
    "            # Check if thumb tip is roughly above or at the same height as the MCP\n",
    "            # relative to the wrist.\n",
    "            if tip.y > mcp.y + (wrist_y - mcp.y) * 0.2: # Simple heuristic\n",
    "                all_fingers_extended = False\n",
    "                # print(f\"{finger_name} not up (y-position check)\")\n",
    "                break\n",
    "\n",
    "            # Calculate angle between MCP-IP-TIP to check for curl\n",
    "            # We'll use 2D projection for simplicity, considering x and y\n",
    "            vec_mcp_ip = np.array([ip.x - mcp.x, ip.y - mcp.y])\n",
    "            vec_ip_tip = np.array([tip.x - ip.x, tip.y - ip.y])\n",
    "\n",
    "            dot_product = np.dot(vec_mcp_ip, vec_ip_tip)\n",
    "            magnitude_mcp_ip = np.linalg.norm(vec_mcp_ip)\n",
    "            magnitude_ip_tip = np.linalg.norm(vec_ip_tip)\n",
    "\n",
    "            # Avoid division by zero\n",
    "            if magnitude_mcp_ip == 0 or magnitude_ip_tip == 0:\n",
    "                continue\n",
    "\n",
    "            angle_rad = np.arccos(dot_product / (magnitude_mcp_ip * magnitude_ip_tip))\n",
    "            angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "            if angle_deg > 160: # Angle close to 180 means relatively straight\n",
    "                 # print(f\"{finger_name} is curled (angle: {angle_deg})\")\n",
    "                 all_fingers_extended = False\n",
    "                 break\n",
    "\n",
    "        else:\n",
    "            # For other fingers, check PIP and DIP joints relative to MCP.\n",
    "            # A common approach is to check if the Y-coordinate of the tip is significantly\n",
    "            # higher than the Y-coordinate of the MCP, and also check joint angles.\n",
    "            mcp = hand_landmarks.landmark[landmarks[0]]\n",
    "            pip = hand_landmarks.landmark[landmarks[1]]\n",
    "            dip = hand_landmarks.landmark[landmarks[2]]\n",
    "            tip = hand_landmarks.landmark[landmarks[3]]\n",
    "\n",
    "            # Check if finger is pointing generally upwards (Y-coordinate check)\n",
    "            # Compare tip's Y with MCP's Y (lower Y means higher in image)\n",
    "            if tip.y > pip.y or pip.y > mcp.y: # If any joint is lower than previous, it's likely bent down\n",
    "                all_fingers_extended = False\n",
    "                # print(f\"{finger_name} not up (y-position check)\")\n",
    "                break\n",
    "\n",
    "            # Calculate angles for PIP and DIP joints\n",
    "            # PIP joint angle (MCP-PIP-DIP)\n",
    "            vec_mcp_pip = np.array([pip.x - mcp.x, pip.y - mcp.y, pip.z - mcp.z])\n",
    "            vec_pip_dip = np.array([dip.x - pip.x, dip.y - pip.y, dip.z - pip.z])\n",
    "            angle_pip = np.degrees(np.arccos(np.dot(vec_mcp_pip, vec_pip_dip) / (np.linalg.norm(vec_mcp_pip) * np.linalg.norm(vec_pip_dip) + 1e-6)))\n",
    "\n",
    "            # DIP joint angle (PIP-DIP-TIP)\n",
    "            vec_pip_dip_2 = np.array([dip.x - pip.x, dip.y - pip.y, dip.z - pip.z]) # Recalculate or reuse\n",
    "            vec_dip_tip = np.array([tip.x - dip.x, tip.y - dip.y, tip.z - dip.z])\n",
    "            angle_dip = np.degrees(np.arccos(np.dot(vec_pip_dip_2, vec_dip_tip) / (np.linalg.norm(vec_pip_dip_2) * np.linalg.norm(vec_dip_tip) + 1e-6)))\n",
    "\n",
    "\n",
    "            if angle_pip > angle_threshold or angle_dip > angle_threshold:\n",
    "                all_fingers_extended = False\n",
    "                # print(f\"{finger_name} not up (angle check: PIP={angle_pip:.2f}, DIP={angle_dip:.2f})\")\n",
    "                break\n",
    "\n",
    "    return all_fingers_extended\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5,\n",
    "    max_num_hands=1) as hands: # Only process one hand for simplicity\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a natural selfie-view display.\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        high_five_detected = False\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "                if is_all_fingers_up(hand_landmarks):\n",
    "                    high_five_detected = True\n",
    "                    break # Only need one hand for high five\n",
    "\n",
    "        # Display detection status\n",
    "        if high_five_detected:\n",
    "            cv2.putText(image, \"HIGH FIVE!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(image, \"No High Five\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        cv2.imshow('MediaPipe Hands - High Five Detector', image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae9f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "con_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
